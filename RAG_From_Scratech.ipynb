{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DYFEKeEWWMSm"
   },
   "source": [
    "# RAG From Scratch â€” Step-by-Step Tutorial\n",
    "\n",
    "This notebook demonstrates how to implement a Retrieval-Augmented Generation (RAG) pipeline from scratch, **without relying on high-level RAG frameworks**.\n",
    "\n",
    "We will build:\n",
    "1. **Document Text Extraction** â€” Read text from PDF files.\n",
    "2. **Chunking with Overlap** â€” Split text into manageable pieces.\n",
    "3. **Embeddings** â€” Convert text chunks into vector representations.\n",
    "4. **Retrieval** â€” Find the most relevant chunks for a given query.\n",
    "5. **Answer Generation (Basic)** â€” Use retrieved chunks to form answers.\n",
    "\n",
    "**Goal:** Learn and understand each step of RAG by coding it ourselves.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Btoy9_kW7E1"
   },
   "source": [
    "## Step 0 â€” Environment & Setup\n",
    "Before starting, let's check our Python version and install required libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g44dVXUrWHv-",
    "outputId": "01302227-61b7-46d1-cee2-8acd83d30378"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.11.9\n",
      "Platform: Windows-10-10.0.26100-SP0\n"
     ]
    }
   ],
   "source": [
    "import sys, platform\n",
    "print(\"Python:\", sys.version.split()[0])\n",
    "print(\"Platform:\", platform.platform())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BWIbSirpXD9R"
   },
   "source": [
    "### Install dependencies\n",
    "We will use:\n",
    "- `pdfplumber` for PDF text extraction\n",
    "- `sentence-transformers` for embeddings\n",
    "- `numpy` for similarity calculations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kHCDAzTIW6Br",
    "outputId": "168f7549-a467-4b5a-b24f-76c8a09131fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfplumber in c:\\users\\shivam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.11.7)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\shivam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (5.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\shivam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.1.1)\n",
      "Requirement already satisfied: pdfminer.six==20250506 in c:\\users\\shivam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pdfplumber) (20250506)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\users\\shivam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pdfplumber) (10.4.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\shivam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pdfplumber) (4.30.0)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\shivam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pdfminer.six==20250506->pdfplumber) (3.4.2)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\shivam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pdfminer.six==20250506->pdfplumber) (45.0.6)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\shivam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (4.55.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\shivam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\shivam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (2.8.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\shivam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\shivam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (1.14.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\shivam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (0.34.4)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\shivam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (4.14.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\shivam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\shivam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.7.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\shivam\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\shivam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\shivam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\shivam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\shivam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shivam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\shivam\\appdata\\roaming\\python\\python311\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\shivam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.7.34)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\shivam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\shivam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\shivam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\shivam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\shivam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\shivam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shivam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shivam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shivam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shivam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.6.15)\n",
      "Requirement already satisfied: pycparser in c:\\users\\shivam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from cffi>=1.14->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.22)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pdfplumber sentence-transformers numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZUA706iCXPCf"
   },
   "source": [
    "## Step 1 â€” Document Text Extraction\n",
    "We start by extracting text from a PDF file using `pdfplumber`.\n",
    "\n",
    "**Inputs:**\n",
    "- `pdf_path` â€” Path to the PDF file.\n",
    "\n",
    "**Outputs:**\n",
    "- `text` â€” Extracted text from all pages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xWzBjxSPXDN5",
    "outputId": "6669b6ff-f9b4-4188-9248-54f0c33d1adc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shivam Kumar\n",
      "Indian Institute of Technology, Roorkee\n",
      "(cid:211) +91-7037162459  shivamkushwaha636@gmail.com  shivam k@amsc.iitr.ac.in  Linkedin (cid:135) Github\n",
      "EDUCATION\n",
      "Indian Institute of Technology, Roorkee 2023 â€“ 2025\n",
      "M.Tech - Applied Mathematics and Scientific Computing CGPA: 7.45\n",
      "Jamia Millia Islamia, New Delhi 2019 â€“ 2021\n",
      "M.Sc. - Applied Mathematics Percentage: 84.08\n",
      "INTERESTS\n",
      "â€¢ AI â€¢ Time Series â€¢ LLM â€¢ GenAI\n",
      "COURSEWORK / SKILLS\n",
      "â€¢ DSA â€¢ Machine Learning â€¢ NLP â€¢ Mathematics\n",
      "â€¢ Soft Computin\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "\n",
    "pdf_path = 'Enter your PDF Path'\n",
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "    text = \"\".join([page.extract_text() for page in pdf.pages])\n",
    "\n",
    "print(text[:500])  # preview first 500 characters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k_zCG0bbXkCh"
   },
   "source": [
    "## Step 2 â€” Generating Unique IDs\n",
    "Sometimes we may want to assign unique IDs to chunks or documents.\n",
    "Here we use `uuid-utils` to generate a random UUID.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rqfFDq0EXknE",
    "outputId": "93000af7-99ea-4665-9456-1e10733d0322"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: uuid-utils in c:\\users\\shivam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.11.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install uuid-utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u0mOMjf-XmyN",
    "outputId": "8aa8dc73-cc47-4d07-9084-84fa6e896acc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f3591049-a7a5-4786-823b-df90821658c5\n"
     ]
    }
   ],
   "source": [
    "import uuid_utils as uuid\n",
    "id = uuid.uuid4()\n",
    "print(str(id))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9DUCRDbFXwlu"
   },
   "source": [
    "## Step 3 â€” Chunking with Overlap\n",
    "We split the document text into smaller chunks, allowing overlaps for context preservation.\n",
    "\n",
    "**Function:** `chunk_overlap(text, chunk_size, overlap)`\n",
    "- `text` â€” Full text\n",
    "- `chunk_size` â€” Length of each chunk\n",
    "- `overlap` â€” Number of overlapping characters between chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "gXtlo7u7Xubl"
   },
   "outputs": [],
   "source": [
    "def chunk_overlap(text, chunk_size, overlap):\n",
    "    \"\"\"\n",
    "    Splits text into chunks of given size with overlaps.\n",
    "\n",
    "    Args:\n",
    "        text (str): Full document text.\n",
    "        chunk_size (int): Size of each chunk in characters.\n",
    "        overlap (int): Number of characters overlapping between chunks.\n",
    "\n",
    "    Returns:\n",
    "        dict: Mapping from chunk index to chunk text.\n",
    "    \"\"\"\n",
    "    chunks = {}\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        chunk_id = str(uuid.uuid4())\n",
    "        end = start + chunk_size\n",
    "        chunks[chunk_id] = text[start:end]\n",
    "        start = end - overlap\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3s2WnDYUYRHz"
   },
   "source": [
    "### Apply chunking\n",
    "We apply our chunking function to the extracted text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kqb4XBSNYOmj",
    "outputId": "3e61a5d2-b73e-419e-fd3c-d9772c5bd26b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7,\n",
       " [('29a18838-074f-4fff-aa4f-ba83e10aabac',\n",
       "   'Shivam Kumar\\nIndian Institute of Technology, Roorkee\\n(cid:211) +91-7037162459  shivamkushwaha636@gmail.com  shivam k@amsc.iitr.ac.in  Linkedin (cid:135) Github\\nEDUCATION\\nIndian Institute of Technology, Roorkee 2023 â€“ 2025\\nM.Tech - Applied Mathematics and Scientific Computing CGPA: 7.45\\nJamia Millia Islamia, New Delhi 2019 â€“ 2021\\nM.Sc. - Applied Mathematics Percentage: 84.08\\nINTERESTS\\nâ€¢ AI â€¢ Time Series â€¢ LLM â€¢ GenAI\\nCOURSEWORK / SKILLS\\nâ€¢ DSA â€¢ Machine Learning â€¢ NLP â€¢ Mathematics\\nâ€¢ Soft Computin'),\n",
       "  ('7777e251-cc4c-4037-9101-829fd316c484',\n",
       "   'chine Learning â€¢ NLP â€¢ Mathematics\\nâ€¢ Soft Computing â€¢ Deep Learning â€¢ Statistics â€¢ Optimization\\nPROJECTS\\nExploratory Data Analysis on FIFA World Cup 2022 Dataset | IIT Roorkee Nov 2023\\nâ€¢ Conducted an in-depth analysis of the FIFA 2022 dataset with a team of four members.\\nâ€¢ Project encompassed data cleaning, feature engineering, data visualization, and comparative analysis.\\nâ€¢ Leveraged dataset analysis to extract actionable insights on performance metrics and emerging trends.\\nRAG and Its Applicat'),\n",
       "  ('80130443-ac78-413e-a8eb-658332166373',\n",
       "   ' metrics and emerging trends.\\nRAG and Its Application in Resume Shortlisting | IIT Roorkee May 2025\\nâ€¢ Developed a Retrieval-Augmented Generation (RAG) system to automate resume shortlisting using\\ncontextual retrieval and LLM-based generation.\\nâ€¢ Integrated vector-based retrieval and large language models to match resumes with job descriptions\\nbased on semantic relevance.\\nâ€¢ Enabled scalable and interpretable shortlisting decisions, improving filtering accuracy and reducing\\nmanual effort.\\nNeural Ne')])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = chunk_overlap(text, chunk_size=500, overlap=50)\n",
    "len(chunks), list(chunks.items())[:3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "btZi1SamYcf2"
   },
   "source": [
    "## Step 4 â€” Load Embedding Model\n",
    "We use `SentenceTransformer` to convert each chunk into a dense vector embedding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493,
     "referenced_widgets": [
      "98cd30595b9b4429abf39063de8ba27e",
      "b53ec4452e044878ad4e81ae66821b88",
      "b2d729737c8b44649246f96f72956a5b",
      "aa2941855f0a4ec893577ff40423d0f7",
      "cd301e50a95f4431a218184e56b7796b",
      "ceb08b756a214d9393a1f605f9781e54",
      "2c53b91c39cd401b80f130b6a3326ac9",
      "0b36923d12934c8dbfac816ba3b6771c",
      "3eac09aecc6c4a9e91b6320e842d3789",
      "46789978a25f4eb5aff31bdf2d64e88d",
      "fd77e04f081b424388e40d3cd70795c1",
      "a48d31266da449409e15248616a0ddeb",
      "10d4588a1d864db782e1ecdf318f8dac",
      "5f979838c9424d10bb0d0c4515f14f69",
      "82a6abcc98bd43a2b574490870341fad",
      "0760013174a647b18839d0d4ca5735c7",
      "3d13d2f0f3154251a1ad688a2d4372cc",
      "afd4ba798673493ba6453afa63f6647f",
      "f165e6ee24824681a0fbf8df20a8aa27",
      "7d8dbc46661c4f27be7e5c9f0e9510c4",
      "5c236c01627a40cbb091b74da4765d12",
      "720f88a7f96f4e70b56f8a2b4204bb2d",
      "d09a693f8b504689ba9e13dfa1353a9f",
      "64e6f7276104421dbd9a3a35081c2b82",
      "75c8974439204ab293a3d5ddb50a173f",
      "64ab5b2cb00848829e1d49ee29d99a0c",
      "ac7160618c574702bfb1cc320bf8475f",
      "f0cd76d6202e45e1adcf269e0c66f1a3",
      "94c1745516e4404ca2677d8c72065951",
      "afacd80a5a4f4b2f83a2e71142856bde",
      "30eed059cfe84d11bf36ad6db1e5737b",
      "4d0267083a2849a99d8041906076fa3b",
      "941a4cc484194f2dae059af75e7b0bb6",
      "c6f4536ae2a140e5968b27f95ed2249b",
      "a5c2900dcf9645158001d0b0aff29a03",
      "5b2c0c1a713b478d9d1b3719157e3351",
      "dd28b36f7ecd4f5297142cc72319c2d3",
      "a61537a24cce494a96bd069bde0375b1",
      "0bd3ba3819154f94bb747dbb2a602ac7",
      "be7972127f9143628bf0ded3b96349e0",
      "2d351b31f0a24a9489500dd634961771",
      "782fcfe4836e4e83a59ada0aa6d0f09f",
      "f49ca0d06b6c4d1d89c45d41c7856f8e",
      "4e36084eaa0e4befa2b5d3d48f63015c",
      "60d42a10e01a48e1bedfb286dedbeca0",
      "0d9bc3913fce4d15bceb4d1ded2b7336",
      "e41c46ee16ce42b4be7394bf30c8a2e6",
      "2e9760b0f24c4b31a36647b0c40f7606",
      "97b97952de5141d28bac0e681c5f5d75",
      "0a7f5319f0c2498fa783c29a70ca7fbf",
      "6442c244a2c44c40b3ba5b43bf345f74",
      "65418a290943481880a8951134d3f03f",
      "0b334cbfec0245aaa7f328ef85ad92d8",
      "f85e4c0a06d44d40ae6d837a2e36fd49",
      "9fe8630c9391425cb1e145d6b9600b47",
      "f365c3bbcd774e8e9db8548980acec51",
      "e0e31471563c4e309baa150ba5f5fd3c",
      "cd058887f18144168f718c02eeca27ac",
      "beb4fe4db53349fda4b7c38f0ac07b82",
      "219cf14f5fd448d1b0efda328ba396ff",
      "581d0a2e56694ad3962dd60902d567cd",
      "29924dc1ec7e49948c1767716cdc0050",
      "cca9c12ba74c46f78adc05fdafe6614b",
      "45d1952553354a7d9ba5df089e4190c3",
      "4d9ea8bf0b3c493f92bdad69a3b23aa9",
      "857ec0e287cd40578749e8c0d89ce0c3",
      "c3c488017f064837895d3ef70a350063",
      "92360d2c59614e8a9785a79e1c178cd3",
      "239761cf2d3d4a169f714385fde077a3",
      "72e5a8321f814a7ebb8fa492498ab6bd",
      "08942653f0534b9e91d35c589ae1ffc8",
      "246367b79f814f0c8f5e92d6e7bd46d5",
      "604bc87d7e724d19a26b94b333ed2eac",
      "c484c5c86ffe4bb1abfa9984508445ef",
      "45ab1d6a0acd461596344b691dd3e6b9",
      "18ef427bad2d410d8965e2f9e704a8e4",
      "95905a62f6d545878a5cbd3ac21caf3c",
      "d3a07b3c433541b3a5967704d1248497",
      "6f3b1b798d784a55b4d7d6e3a36712b9",
      "b3ffa1dd16434ff1a690fb7152d39646",
      "8ac11bb8deb74ad98ce329f293802cb2",
      "859fe03bd6be439185ccb8d71eacc80a",
      "420c088d25164dc5908e7991521e657b",
      "23c1bece432f43abbbd73bfe0338c9b5",
      "d9e7401646e34671abb1de0ed92b57e9",
      "a38880476ce2495f833539b51bac594f",
      "580ff23a953a405e922da7605f4883cf",
      "15e6e502bdb4402894964bbc3e535db6",
      "56061d9aa79146bc9183bb95b0f6b3ad",
      "b04a6b4acd7649a4899081471f294858",
      "2223f945ba704d00a58ca6b58fd1f1d4",
      "68c9869ab3044243863e07f937f46fae",
      "77342e893e8848f1980b2f7ab71e1416",
      "57cf976317d64e4ea4cdad6c108e5500",
      "f990a529b08a4c478b20dd13a72fab96",
      "be0e84cfd0084dbf8acbec70442b7e2c",
      "e4433305055a45eabf1d48df3a4c9206",
      "3eb2712f439e4602a13d6ba60ea44033",
      "5429cfffbe6d4580889e8377273f3910",
      "019761872e084bc9b035b40dff525aa0",
      "4a3457026d0a4ec1b9388ae25bd0f458",
      "92456928ddb640688b7f476377485833",
      "d491e6f7c40e443599c9656637d13f10",
      "e20575977dc142be80f8edc2e9dea61f",
      "3ffaa1fbe5c947c19527b42408454a60",
      "67ef6b43b75f48bdad23ceb371a549ae",
      "91a7b7916d374364b93d0837edc7b98d",
      "ba7151d004a44238bb6a1cc38b1cc505",
      "a8442240c7084f1c85454581f7019d97",
      "bc3956a70ead4fadaa15111dd832dcbf",
      "f18d1a6b29184dcea6fc18ef8c1587f2",
      "4122fca9310b4ace8085d16463fce015",
      "38914b80f2d046e793eb4e02c1eb4516",
      "e6601d2416b34b63a23f5350ef1068fa",
      "5bcef9d29f744af0b6384ae6e54e3470",
      "52788f78d9b54a2998628d1179476c41",
      "f1cb69d24eca4997bc44159b393d53aa",
      "93bf84911c414b22be420f3c834ba522",
      "0298ce7885ae4ed9aaba1bf55096e6b7",
      "e04b8d207146420cbffe98c997fca316",
      "95ebb0234cf64b459c50757dad35da4e"
     ]
    },
    "id": "eZbp-Aj1YVf0",
    "outputId": "9a1e0d46-ce42-4ee1-8d9b-68d88cdbad4d"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nt3brDHXYhcR"
   },
   "source": [
    "### Generate embeddings for all chunks\n",
    "We pass each chunk to the embedding model and store the resulting vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "J34HE3VhYfH8"
   },
   "outputs": [],
   "source": [
    "def embedd_chunks(chunks):\n",
    "    \"\"\"\n",
    "    Converts text chunks into embeddings using SentenceTransformer.\n",
    "\n",
    "    Args:\n",
    "        chunks (dict): Mapping from chunk index to text.\n",
    "\n",
    "    Returns:\n",
    "        dict: Mapping from chunk index to embedding vector.\n",
    "    \"\"\"\n",
    "    chunk_embeddings = {}\n",
    "    for idx, chunk in chunks.items():\n",
    "        chunk_embeddings[idx] = model.encode(chunk)\n",
    "    return chunk_embeddings\n",
    "\n",
    "embeddings = embedd_chunks(chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SOvZOjNQY6Pn"
   },
   "source": [
    "## Step 5 â€” Retrieve Relevant Chunks\n",
    "Given a query, we find the top-k most relevant chunks using cosine similarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "OBZZ8yqaY0-1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def retrieve_chunks(query, k):\n",
    "    \"\"\"\n",
    "    Retrieves the top-k most relevant chunks for a query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The user query.\n",
    "        k (int): Number of chunks to retrieve.\n",
    "\n",
    "    Returns:\n",
    "        dict: Mapping of chunk index to similarity score.\n",
    "    \"\"\"\n",
    "    query_embedd = model.encode([query])[0]\n",
    "    similarity = {}\n",
    "    for idx, emb in embeddings.items():\n",
    "        sim = np.dot(query_embedd, emb) / (np.linalg.norm(query_embedd) * np.linalg.norm(emb))\n",
    "        similarity[idx] = sim\n",
    "    sorted_similarity = sorted(similarity.items(), key = lambda x: x[1], reverse = True)\n",
    "    top_chunks = [chunks[id] for id, _ in sorted_similarity[:k]]\n",
    "    return top_chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nLPrrTapZpJQ"
   },
   "source": [
    "### Test retrieval with a sample query\n",
    "We query the system and print the top-k chunks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LxuGFu3RZmA2",
    "outputId": "99798f05-e67d-4f22-8a25-0238bb49f4a0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Shivam Kumar\\nIndian Institute of Technology, Roorkee\\n(cid:211) +91-7037162459  shivamkushwaha636@gmail.com  shivam k@amsc.iitr.ac.in  Linkedin (cid:135) Github\\nEDUCATION\\nIndian Institute of Technology, Roorkee 2023 â€“ 2025\\nM.Tech - Applied Mathematics and Scientific Computing CGPA: 7.45\\nJamia Millia Islamia, New Delhi 2019 â€“ 2021\\nM.Sc. - Applied Mathematics Percentage: 84.08\\nINTERESTS\\nâ€¢ AI â€¢ Time Series â€¢ LLM â€¢ GenAI\\nCOURSEWORK / SKILLS\\nâ€¢ DSA â€¢ Machine Learning â€¢ NLP â€¢ Mathematics\\nâ€¢ Soft Computin',\n",
       " 'T. LTD. Oct-2019 To Till Now\\nâ€¢ To answer questions of mathematics posted by students on the QA board while maintaining a high level of\\nacademic integrity.\\nFreelance Expert | BARTLEBY Apr-2022 To Aug-2022\\nâ€¢ To provide students with step-by-step solutions for textbook problems and homework questions online.\\nVolunteer | 12th international conference on soft computing and problem solving IIT Roorkee Aug-2023\\nâ€¢ Volunteered during 12th international conference on soft computing and problem solving hel',\n",
       " 'th decision-making.\\nTECHNICAL SKILLS\\nLanguages: Python, SQL\\nSoftware Packages: Numpy, Pandas, Seaborn, Matplotlib, SciPy, Scikit-Learn, TensorFlow, Keras, PyTorch\\nCODING PLATFORMS\\nâ€¢ Solved 200+ Problems on Leetcode. \\nâ€¢ Solved 500+ Problems on GeeksforGeeks. \\nACHIEVEMENTS\\nâ€¢ Qualfied GATE(MA) exam with AIR 633 in 2023\\nâ€¢ Qualified JAM(MA) exam in 2019 with AIR 1103\\nPOSITIONS, RESPONSIBILITES & EXTRA CURRICULARS\\nSubject Matter Expert | CHEGG INDIA PVT. LTD. Oct-2019 To Till Now\\nâ€¢ To answer questions']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is CGPA of the candidate?\"\n",
    "k = 3\n",
    "retrieve_chunks(query, k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7j-BL1Loa4sb"
   },
   "source": [
    "## Step 7 â€” LLM Answer Generation with Groq API\n",
    "\n",
    "Now that we can retrieve the most relevant chunks, let's use an LLM to generate an answer.  \n",
    "We'll use Groq's `llama-3.3-70b-versatile` model, passing in both the **query** and the **retrieved context**.\n",
    "\n",
    "**Process:**\n",
    "1. Retrieve top-k relevant chunks from our knowledge base.\n",
    "2. Combine them into a single context string.\n",
    "3. Pass query + context to Groq LLM for final answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WO1njvOPa4Pj",
    "outputId": "22e32047-af4b-4e8a-fdd6-a2d010569d3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: groq in c:\\users\\shivam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.31.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\shivam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from groq) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\shivam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\shivam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from groq) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\shivam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from groq) (2.11.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\shivam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in c:\\users\\shivam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from groq) (4.14.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\shivam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\shivam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\shivam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\shivam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\shivam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\shivam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\shivam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (0.4.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yy-6eChFbHR-"
   },
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "\n",
    "# Initialize Groq client with your API key\n",
    "api_key = \"Enter Your GROQ API Key\"  # Replace with your actual key\n",
    "client = Groq(api_key=api_key)\n",
    "\n",
    "def generate_answer(query, k=3):\n",
    "    \"\"\"\n",
    "    Generates an answer using Groq LLM based on retrieved chunks.\n",
    "\n",
    "    Args:\n",
    "        query (str): The user's question.\n",
    "        k (int): Number of top chunks to use as context.\n",
    "\n",
    "    Returns:\n",
    "        str: The LLM-generated answer.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Retrieve top-k chunks\n",
    "    retrieve_chunk = retrieve_chunks(query,k=3)\n",
    "\n",
    "    # Step 2: Build context from retrieved chunks\n",
    "    retrieve_context = \"\\n\".join(retrieve_chunk)\n",
    "\n",
    "    # Step 3: Create prompt for the LLM\n",
    "    prompt = f\"\"\"\n",
    "    You are an help full assistent, who help other to answer query from the given context.\n",
    "    query: {query}\n",
    "    context: {retrieve_context}\n",
    "\n",
    "    If you dont find answer from the context, politely say so.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 4: Get completion from Groq LLM\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            # Set an optional system message. This sets the behavior of the\n",
    "            # assistant and can be used to provide specific instructions for\n",
    "            # how it should behave throughout the conversation.\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant.\"\n",
    "            },\n",
    "            # Set a user message for the assistant to respond to.\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "\n",
    "        # The language model which will generate the completion.\n",
    "        model=\"llama-3.3-70b-versatile\"\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NSnEBi98b9m9",
    "outputId": "bbaa7c25-bce2-47c4-ae90-e135b5acf21f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The CGPA of the candidate, Shivam Kumar, is 7.45, as mentioned in the EDUCATION section of the context, specifically for his M.Tech in Applied Mathematics and Scientific Computing at the Indian Institute of Technology, Roorkee.\n"
     ]
    }
   ],
   "source": [
    "# Example test\n",
    "query = \"What is the CGPA of the candidate?\"\n",
    "print(generate_answer(query, k=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The educational information of the candidate, Shivam Kumar, is as follows:\n",
      "\n",
      "* M.Tech in Applied Mathematics and Scientific Computing from the Indian Institute of Technology, Roorkee (2023-2025) with a CGPA of 7.45.\n",
      "* M.Sc. in Applied Mathematics from Jamia Millia Islamia, New Delhi (2019-2021) with a percentage of 84.08.\n",
      "\n",
      "This information is available in the \"EDUCATION\" section of the context.\n"
     ]
    }
   ],
   "source": [
    "# Example 2 test\n",
    "query = 'Give the educational information of this candidate'\n",
    "print(generate_answer(query, k=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, I couldn't find any direct information related to integration and differentiation in the context of decision-making. The context appears to be a resume or a personal profile, highlighting the individual's educational background, technical skills, and work experience.\n",
      "\n",
      "However, I can infer that the individual has a strong background in mathematics, particularly in applied mathematics, and has worked with various software packages and coding platforms. They also have experience in machine learning, NLP, and soft computing, which may involve concepts related to integration and differentiation.\n",
      "\n",
      "If you're looking for information on integration and differentiation in a specific context, such as calculus or mathematical modeling, I'd be happy to try and provide a general overview. But if you're looking for a direct connection to decision-making, I'm afraid I couldn't find any relevant information in the provided context. Would you like me to try and provide a general explanation of integration and differentiation?\n"
     ]
    }
   ],
   "source": [
    "# Example 3 test\n",
    "query = 'Tell me about integration and differentiation'\n",
    "print(generate_answer(query, k=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h4Tgg-1waRQC"
   },
   "source": [
    "## ðŸ“Œ Summary & Next Steps\n",
    "\n",
    "In this notebook, we built a **Retrieval-Augmented Generation (RAG)** system from scratch:\n",
    "\n",
    "1. **Document Loading** â€” Extracted text from a file for knowledge base creation.  \n",
    "2. **Chunking** â€” Split the text into overlapping chunks for better retrieval.  \n",
    "3. **Embedding** â€” Converted each chunk into a vector representation using a sentence transformer model.  \n",
    "4. **Similarity Search** â€” Found the most relevant chunks for a query using cosine similarity.  \n",
    "5. **Retrieval Pipeline** â€” Built a function to fetch top-k chunks as context.  \n",
    "6. **LLM Integration** â€” Used Groq's `llama-3.3-70b-versatile` model to generate final answers from retrieved context.  \n",
    "\n",
    "---\n",
    "\n",
    "### âœ… What We Achieved\n",
    "- Created a **minimal but complete** RAG pipeline without heavy frameworks.\n",
    "- Kept the process transparent so each step is easy to understand and modify.\n",
    "- Enabled the system to **politely handle** cases where the answer is not found in the context.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸš€ Next Steps\n",
    "- **Switch to a Vector Database** (FAISS, ChromaDB, Weaviate) for faster and scalable retrieval.\n",
    "- Experiment with **different embedding models** for better semantic matching.\n",
    "- Add **multi-document support** to handle a larger knowledge base.\n",
    "- Fine-tune or prompt-engineer the LLM for domain-specific tasks.\n",
    "- Deploy as an **API or web app** for interactive querying.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X-oDRT6wZs0m"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
